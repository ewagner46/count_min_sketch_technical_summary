\documentclass[11pt]{article}
\usepackage{epsfig}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{theorem}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{bm}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\setlength\parindent{0pt}
\hspace{-2.5pt}}
\newcommand{\wontfix}{\rlap{$\square$}{\large\hspace{1pt}\xmark}}
 \usepackage[parfill]{parskip} 
\usepackage{mathtools}

\newcommand{\sq}{\mathit{Q}_i}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\hypersetup{
    colorlinks   = true,
    linkcolor    = magenta
}

\newcommand{\ra}{\rightarrow}

\title{Technical Summary: ``An Improved Data Stream Summary: The Count-Min Sketch and its Applications'' 
by Cormode and Muthukrishnan}
\author{Ravi Gaddipati, Matthew Ige, and Emily Wagner}
\begin{document}
\maketitle
\section{Problem Statement and Overview}
Consider a vector $\vec{a}(t) = [a_1(t), \dots, a_i(t), \dots a_n(t)]$ which evolves with time.
Initally, $\vec{a}$ is the zero vector $\vec{0}$.  We represent the $t$th update as $(i_t, c_t)$,
which modifies the vector as follows:
\begin{align}
    a_{i_t}(t) = a_{i_t}(t - 1) + c_t, \\
    a_{i'}(t) = a_{i'}(t - 1), \, i' \neq i_t
\end{align}
The update $(i_t, c_t)$ modifies the $i$th element by adding $c_t$ to it.
For all other $a_{i'}$, the vector remains unchanged. 

This vector and its updates represent some stream of data that evolves with
time. There are 2 main models for such a stream:
\begin{enumerate}
    \item \textbf{cash-register case:} $c_t > 0$, so every vector element is monotonically
    increasing.
    \item \textbf{turnstile case:} $c_t$ can also be negative.  There are two subcases:
    \begin{enumerate}
        \item \textbf{non-negative turnstile:} $(i_t, c_t)$ will never cause vector elements
        $a_{i_t}$ to dip below zero. This guarantee can be provided by the application.
        \item \textbf{general turnstile:} vector elements $a_{i_t}$ may become negative.
    \end{enumerate}
\end{enumerate}

The basic problem this work addresses is to summarize or calculate certain characteristics
about the stream. There are 2 main constraints. First, the space used by such algorithms should be small; at most polylogarithmic
in $n$.  This compressed version of the data is called a \textbf{sketch}.  Second, updates to the sketch should be processed quickly.  Since we are sublinear in input space,
we will have to approximate almost any function we want to compute over $\vec{a}$, but
we still want to specify an approximation parameter $\varepsilon$ and bound the probability
of error by $\delta$.  

The \textbf{count-min sketch} addresses these requirements and allows us to calculate several characteristics
of a stream using a single data structure.  It can be used to approximate three types of \textbf{queries}:
\begin{enumerate}
    \item \textbf{point query:} $\sq(i)$ is an approximation of the vector 
    element $a_i(t)$
    \item \textbf{range query:} $\sq(l, r)$ is an approximation of 
    $\sum_{i = l}^{r}a_i$. 
    \item \textbf{inner product query:} $\sq(\vec{a}, \vec{b})$ approximates
    $\vec{a} \odot \vec{b} = \sum_{i = 1}^{n} a_i b_i$
\end{enumerate}

These queries have a variety of applications: point and range queries are used in summarizing
the data distribution and the inner product query can be used to estimate the join size of 
relations. 

In addition, these queries can be used for more complex data stream functions:

\begin{enumerate}
    % not integers
    \item \textbf{$\phi$-quantiles:} The $\phi$-quantiles of the cardinality
    \begin{align}
        ||\vec{a}||_1 = \sum_{i = 1}^{n}|a_i(t)|
    \end{align}
    are the values that split a multiset of integers $I \in [1 \dots n]$ into $\phi$ groups based on
    their rank $R$.  For example, the median of a set of data is the 2-quantile. 
    
    To obtain the $\phi$-quantiles, the elements are sorted and split as
    follows.  The $k$th quantile is the element(s) with rank $R =
    k\phi||\vec{a}||_1$ for $k = 0, \dots, 1/\phi$.  The
    $\varepsilon$-approximation for the $\phi$-quantiles accepts as the $k$-th
    quantile any integer with rank $(k\phi - \varepsilon)||\vec{a}||_1 \leq R
    \leq (k\phi + \varepsilon)||\vec{a}||_1$ for a given $\varepsilon < \phi$.
    \item \textbf{Heavy hitters:}
\end{enumerate}

\section{State of the art}
Sketches have been commonly used to help analyze large streams of data. These sketches allow for the following types of queries: $L_1$ and $L_2$ norms, number of distinct items in a sequence, join size of relations, range sum queries, and more. Although these data structures have proved to be very powerful, there are a few key drawbacks that limit the effectiveness of these sketches. A few of the drawbacks are as follows:
\begin{enumerate}
    \item Common sketches typically have a $\Omega(\frac{1}{\epsilon^2})$ multiplicative factor. For common uses of $\epsilon$, like 0.1 or 0.01, this scaling can quickly become too expensive.
    \item Many sketches take linear time (in the size of the sketch) to do a single update. 
    \item Many sketches require $p$-wise independent hash functions. This is not trivial, and is especially difficult in hardware applications.
    \item Some sketches can only handle one particular query.
    \item Many sketches use analysis that hides large constants.
\end{enumerate}
The count-min sketch reduces these problems in the following ways:
\begin{enumerate}
    \item CMS uses space proportional to $\frac{1}{\epsilon}$
    \item Update time is sublinear with respect to the size
    \item CMS requires only pairwise independent hash functions
    \item The sketch can answer several queries and has numerous applications
    \item All constants are explicit and small.
\end{enumerate}
In particular, notice that the space has been reduce from $\frac{1}{\epsilon^2}$ to $\frac{1}{\epsilon}$ using a CMS, and the time for an update has been reduced from $\frac{1}{\epsilon^2}$ to 1.
\section{How to make a count-min sketch}
Note $e$ is the base of the natural logarithm function, $\ln$. The count-min
sketch is an array with $w$ columns and $d$ rows where:
\begin{align}
    w = \ceil{e/\varepsilon}\\
    d = \ceil{\ln{1/\delta}}
\end{align}
for a given accuracy parameter $\varepsilon$ and probability guarantee $\delta$.
We need $d$ pair-wise independent hash functions
\begin{align}
    h_1 \dots h_d : \{1 \dots n\} \ra \{1 \dots w\} 
\end{align}
The sketch starts with every array entry being 0. The sketch is updated when the $t$th update $\{a_{i_t}, c_t\}$
arrives as follows
\begin{align}
    count[j, h_j(i_t)] \leftarrow count[j, h_j(i_t)] + c_t     
\end{align}
In other words, upon receiving an update $(i_t, c_t)$:
\begin{enumerate}
    \item for each row $1 \leq j \leq d$
    \begin{enumerate}
        \item add $c_t$ to the $h_j(i)$th column.
    \end{enumerate}
\end{enumerate}

\section{Approximate Point Queries using CM sketches}

\subsection{Non-Negative Case}
\subsubsection{Algorithm}
For the non-negative case (either the non-negative turnstile or cash-register case),
the algorithm to estimate $\hat{a}_i(t) = \sq(i)$ is as follows: 
\begin{align}
    \sq(i) = \hat{a}_i = \text{min}_j count[j, h_j(i)]
\end{align}
In other words for a given $i_t = i$, $\hat a_i =$ \texttt{Estimate\_Point\_Query(CM\_Sketch($\vec{a}, t$), i )}:

\begin{lstlisting}[escapeinside={(*}{*)}]
Estimate_Point_Query(CM_Sketch((*$\vec{a}$*), t), i) 
    make empty multiset S;
    for each row j, 1 (*$\leq$*) j (*$\leq$*) d:
        S = S (*$\cup$*) count[j, h_j(i)]
    return min(S);
\end{lstlisting}

\textbf{Theorem 1:} The estimate $\hat a_i$ has the following guarantees:
\begin{enumerate}[label=\textnormal{(\arabic*)}]
    \item $a_i \leq \hat{a}_i$
    \item with probability at least $1 - \delta$, 
    \begin{align}
        \hat{a}_i \leq a_i + \varepsilon ||\vec{a}||_1 
    \end{align}
\end{enumerate}
\subsubsection{Proof of Theorem 1}
\textbf{Define} an indicator variable $I_{i, j, k}$ as follows:
\begin{align}
    I_{i, j, k} = 
    \begin{cases}
        1, & \text{ if } h_j(i) = h_j(k) \text{ and } i \neq k \\
        0, & \text{otherwise}
    \end{cases}
\end{align}
$I_{i, j, k}$ is 1 if $i$ and $k$, $i \neq k$, have a collision under hash function $h_j$.

Since, to begin with, we chose all $h_j$ from a 2-universal hash family
with range $1 \leq h_j \leq d = \ceil{\frac{e}{\varepsilon}}$ 
we know that, for $i \neq k$,
\begin{align}\label{eq:collision-prob}
    Pr(h_j(i) = h_j(k)) \leq \frac{1}{\text{range}(h_j)} = 1/\ceil{\frac{e}{\varepsilon}} = \ceil{\frac{\varepsilon}{e}}
\end{align}
% how do they get rid of ceil??
by the properties of a 2-universal has function and by our choice of hash function
range.

\textbf{Define} $X_{i, j}$ as follows:
\begin{align}
    X_{i, j} = \sum_{k = 1}^{n}I_{i, j, k} a_k
\end{align}
In other words, $X_{i, j}$ is the sum of all corresponding vector values whose indices
collide with the index of $a_i$ under $h_j$. Trivially, we can then state that
\begin{align}\label{eq:count-def}
    count[j, h_j(i)] = a_i + X_{i, j} 
\end{align}
This is clearly true as the column that $i$ hashes to under $h_j$
contains the real value $a_i$ after update $t$, combined with all of the other
$a_k$, $k \neq i$ after update $t$ that hash to that same spot in that row $j$ under
$h_j$.   Now, Theorem 1.a follows because we are only considering the case where vector
elements are non-negative and thus
\begin{align}
    a_i \leq \hat a_i = a_i + X_{i, j}
\end{align}
for any possible $X_{i, j}$

Now, to show 1.b, we evaluate:
\begin{align}
    E(X_{i, j}) = E\left(\sum_{k = 1}^n I_{i, j, k} a_k\right) 
\end{align}
by definition of $X_{i, j}$. The linearity of expectation states that
\begin{align}
    E(\sum_{i = 1}^{n} c_i X_i) = \sum_{i = 1}^{n} c_i E(X_i)
\end{align}
where $X_i$s may be dependent. Applying linearity of expectations here, we have:
\begin{align}
    E(\sum_{k = 1}^{n} a_k I_{i, j, k}) = \sum_{k = 1}^{n} a_k E(I_{i, j, k}) 
\end{align}
% why less than equals, why not just equals?
We calculated $E(I_{i, j, k}) \leq \ceil{\frac{\varepsilon}{e}}$ above, so 
we have
\begin{align}
    \sum_{k = 1}^{n} a_k E(I_{i, j, k})  \leq\sum_{k = 1}^{n} a_k \ceil{\frac{\varepsilon}{e}}
\end{align}
and finally, since $||\vec{a}||_1 = \sum_{k = 1}^{n} |a_k| = \sum_{k = 1}^{n} a_k$
(all $a_k$ are assumed to be non-negative)
\begin{align}
    \sum_{k = 1}^{n} a_k \ceil{\frac{\varepsilon}{e}} = \ceil{\frac{\varepsilon}{e}} ||\vec{a}|||_1 \\
    \implies E(X_{i, j}) \leq \ceil{\frac{\varepsilon}{e}} ||\vec{a}||_1
\end{align}
% why less than equals, why not just equals?
Now that we know that this is true in expectation, we use Markov to bound the tail bounds:
\begin{align}
    Pr[X \geq a] = \frac{1}{a}E(X) 
\end{align}
Let $X = \hat{a}_i$, and choose $a = a_i + \varepsilon||a||_1$.  Then,
\begin{align}
    Pr[\hat{a}_i > a_i + \varepsilon ||a||_1 = Pr[\forall_{j \cdot} count[j, h_j(i)] > a_i + \varepsilon ||\vec{a}||_1] 
\end{align}
since the probability that all values are greater than $a_i + \varepsilon||\vec a||_1$ is equivalent 
to the probability that the minimum of all values is greater than $a_i + \varepsilon||\vec a||_1$,
which is the definition of $\hat a_i$. Then, as stated in equation \ref{eq:count-def}, we have
\begin{align}
    Pr[\forall_{j \cdot} count[j, h_j(i)] > a_i + \varepsilon ||\vec{a}||_1] = Pr[\forall_{j \cdot} a_i + X_{i, j} > a_i + \varepsilon ||\vec{a}||_1]
\end{align}
Then, subtracting $a_i$ from both sides, and using equation \ref{eq:collision-prob} on the $\varepsilon ||\vec{a}||_1$ term, we have:
\begin{align}
    Pr[\forall_{j \cdot} a_i + X_{i, j} > a_i + \varepsilon ||\vec{a}||_1]  = Pr[\forall_{j \cdot} X_{i, j} > e E(X_{i, j})]
\end{align}
Finally, applying the bounds given by Markov's inequality, we have:
\begin{align}
      Pr[\forall_{j \cdot} X_{i, j} > e E(X_{i, j})] \leq \frac{E(X_{i, j})}{a_i + \varepsilon||a||_1} < e^{-d} < \delta
\end{align}
This estimate is calculated in $O(\ln \frac{1}{\delta})$ as the minimum of a multiset can be taken in linear time.
The space complexity is $(2 + \frac{e}{\varepsilon})\ln \frac{1}{\delta}$ words.

All previous analyses of sketch algorithm use Chebyshev in their estimation analysis, yielding a dependency on
$\frac{1}{\epsilon^2}$ for the space complexity.  Using Markov in this analysis yields a tighter bound,
with a dependency $\frac{1}{\epsilon}$.

Considering only the non-negative case here confers 2 advantages as we can take the minimum of the estimate,
rather than the median.  This allows us only one-sided error, and allows us to calculate the failure
probability, instead of using Chernoff bounds, yielding smaller constants than prior work.

\subsection{General Case}
\subsubsection{Algorithm}
The algorithm is identical to the non-negative case, except that we take the median of the multiset:
\begin{align}
    \sq(i) = \hat{a}_i = \text{median}_j count[j, h_j(i)]
\end{align}

\textbf{Theorem 2:} With probability $1 - \delta^{1/4}$,
\begin{align}
    a_i - 3\varepsilon||\vec{a}||_1 \leq \hat{a}_i \leq a_i + 3\varepsilon||\vec{a}||_1. 
\end{align}
The time and space complexity are the same as the non-negative case: $O(\ln
\frac{1}{\delta})$ time and $(2 + \frac{e}{\varepsilon})\ln \frac{1}{\delta}$
words, respectively.

\section{Inner Product Query}
	\subsection{Algorithm}
		The estimate the inner product $\mathit{Q}(a,b)$ for non-negative vectors $a$ and $b$ is $\widehat{a \cdot b} = \min_j(\widehat{a \cdot b})_j$, where $(\widehat{a \cdot b})_j = \sum_{k=1}^w count_a[j,k] * count_b[j,k]$. The algorithm to compute the inner product follows from point queries.
		\textbf{Theorem 3:} The inner product $a \cdot b$ is less than or equal to the estimated inner product such that,
		\begin{align}
		a \cdot b \leq \widehat{a \cdot b} \\
		Pr(\widehat{a \cdot b} \leq a \cdot b + \epsilon||a||_1||b||_1) = 1-\delta
		\end{align}

	\subsection{Proof of Theorem 3}
		We represent the estimated inner product as the sum of the true inner product and collisions. By the pairwise independence of $h$,
		\begin{align}
		(\widehat{a \cdot b})_j &= \sum_{i=1}^n a_ib_i + \sum_{p \neq q, h_j(p)=h_j(q)} a_pb_q \\
		E\left[(\widehat{a \cdot b})_j - a \cdot b\right] &= E\left[\sum_{p \neq q, h_j(p)=h_j(q)} a_pb_q\right] \\
		&= Pr(h_j(p) = h_j(q))a_pb_q \\
		&\leq \sum_{p \neq q} \frac{\varepsilon}{\mathrm{e}}a_pb_q \\
		&\leq \frac{\varepsilon}{\mathrm e}||a||_1||b||_1
		\end{align}
		Where this follows by the reduction of the inner product computation as a series of point queries as outlined above. Applying the markov inequality,
		\begin{align}
		Pr(\widehat{a \cdot b} - a \cdot b > \varepsilon ||a||_1||b||_1) &\leq \frac{E[\widehat{a \cdot b} - a \cdot b]}{||a||_1||b||_1} \\
		&\leq \frac{\frac{\varepsilon}{\mathrm e}||a||_1||b||_1}{\epsilon||a||_1||b||_1} \\
		& \leq 1/e \\
		&\leq \delta
		\end{align}

\section{Range Query}
A range query, denoted as: $\mathcal{Q}(l,r)$, requests the value:
\begin{align*}
    a[l,4] = \Sigma_{i=l}^r a_i
\end{align*}
And we denote our estimator based on the CM sketch as: $\hat a[l,r]$.\\\\
    \textbf{Procedure for estimation:}\\
    Keep $log_2n$ CM sketches. A single range query can be converted into at most $2log_2n$ dyadic range queries, and each dyadic range query can be converted into a single point query. A CM sketch is kept for every dyadic range, and we do a single point query for every dyadic range that makes up the desired range. The sum of all of the point queries is the result, $\hat a[l,r]$.\\\\
    \textbf{Theorem 4: } $a[l,r] \leq \hat a[l,r]$. With probability at least $(1-\delta)$, $\hat a[l,r] \leq a[l,r] + 2\epsilon logn ||\bm{a}||_1$.\\\\
    
    \textbf{Proof: } First, consider the concept of using dyadic ranges: Any range can be converted into a equivalent set of dyadic ranges, in the form of $[x2^y+1, ... , (x+1)2^y]$. For any range we desire, we can equate the construct of a set of dyadic ranges to constructing a tree: the root is the whole range, then its children are each half of the range, and this continues until we hit the leaves, which are sets of singular elements. In the context of a range query, we keep a CM sketch for each level in the tree, and every node in that level represents a value in that CM sketch. When we get an update, we update every CM sketch. Thus, the number of CM sketches we keep is the height of the tree, which is $log_2n$. When we get a range query, we can traverse the tree and, at each level, our range query will be covered by at most one node in the level. At worst, we must travel down the entire height of the tree twice, once for the lower bound of the range, and once for the upper bound of the range. Thus, we cover $log_2n$ for each bound, which means $2log_2n$ nodes are touched. Because each of these nodes can be queried as a single point query, we simply sum up, at most, $2log_2n$ point queries. We can extend the error bound of a single point query to find the error bound of our range query. Recall that a single point query errors by at most $\epsilon ||\bm{a}||_1$, with probability $\delta$. Because each of these point queries that we used are taken from a separate CM sketch, they are all independent. Thus, we can simply add the errors together, and therefore, we error by at most $2\log_2n(\epsilon||\bm{a}||_1)$, and with the same probability, $\delta$.

\section{Applications}

\subsection{Quantiles in the turnstile model}
The count-min sketch can be used to calculate the $\phi$-quantiles in the turnstile model.
Prior work (21) shows that $\phi$-quantiles can be approximated using range sums. This is done
as follows:
\begin{enumerate}
    \item For each $k \in {1, 2, \dots, 1/\phi}$
    \begin{enumerate}
        \item Find a range sum such that $a[1, r] = k\phi||\vec{a}||_1$. Find $r$ using a binary
        search on the possible range sums $a[1, \hat{r}]$, where $\hat{r} \in \{1, 2, \dots, n\}$.  $r$ is the
        $\varepsilon$-approximation for the $k$th $\phi$-quantile.
    \end{enumerate}
\end{enumerate}
The method in [21] uses Random Subset Sums to approximate range sums.  If instead a count-min
sketch is used to approximate the range sums, better results follow.  

To use a count-min sketch to approximate quantiles in the turnstile model, $\log n$ sketches are kept,
one for each dyadic range. Each sketch gets accuracy parameter $\varepsilon/\log n$ for an overall accuracy
bounded by $\varepsilon$. Each sketch gets probability guarantee $\delta\phi/\log n$, for
an overall probability guarantee of $\delta$ for all $1/\phi$ quantiles. Then, we have the following:

\textbf{Theorem 5:} $\varepsilon$-approximate $\phi$-quantiles can be found with probability at least
$1 - \delta$ by keeping a data structure with space $O\left(\frac{1}{\varepsilon}
\log^2(n) \log \left(\frac{\log n}{\phi \delta}\right)\right)$ The time for each insert or delete operation is
$O\left(\log(n) \log \left(\frac{\log n}{\phi \delta}\right)\right)$, and the time to find each quantile on demand 
is $O\left(\log(n)\log\left(\frac{\log n}{\phi \delta}\right)\right)$

This improves the existing query time and update by a factor of more than $\frac{34}{\varepsilon^2} \log n$
The space requirements are improved by a factor of at least $\frac{34}{\varepsilon}$. 

\subsection{Heavy Hitters}
A potential application of the count min sketch is the heavy hitters problem. Given an input $A$ of length $n$ and a parameter $k$ we wish to return values that occur in the input at least $n/k$ times. There may be at most $k$ such heavy hitters, though there also may be none. This sketch provides a convenient way to find the heavy hitters of some data stream with a large input size such that maintaining a counter for each unique element is not feasible. We look at two cases:
\begin{enumerate}
\item The cash register case: elements are only added
\item The turnstile case: elements can be added or removed
\end{enumerate}

\subsubsection{Cash Register Case}
Recall that the cash register case is for positive updates only. We can easily obtain $||\bm{a}||$ at any given point in time because it is simply: $\Sigma_{i=1}^t c_i$.We define a $\phi$ heavy hitter if the estimation of the point query, $Q(i_t) =  \hat a_{it} \geq \phi ||\bm{a}(t))||$. We can do this by maintaining a heap to store the items above the $phi||\bm{a}(t)||$ threshold. On any update, we check the lowest value in the heap, and if the update would be greater than the lowest item, we replace it in the heap. When we are finished with the stream, we do a final scan of all items in the heap, and return the ones which have a value over $\phi||\bm{a}||_1$.\\\\
\textbf{Theorem 6: } We can identify the heavy hitters of a sequence of length $||\bm{a_i}||_1$ using space $O(\frac{1}{\epsilon}log(\frac{||\bm{a}||}{\delta}))$ and time $O(log(\frac{||\bm{a}||}{\delta}))$. Every item which occurs with count $\phi ||\bm{a}||_1$ is output, and with probability $(1-\delta)$, no items whose count is less than $(\phi - \epsilon)||\bm{a}||_1$ is output.\\\\
\textbf{Proof idea:}\\
Because we have only positive updates, and a CM sketch will only over estimate, it is not possible to miss any heavy hitter. Thus, we will never omit any heavy hitters. The bound on the error of outputting a non heavy hitter comes from the fact that a single point query outputs the estimate count of an item within $\epsilon$ of the actual value, with probabiltiy $(1-\delta)$. Because the heavy hitter relies purely on the point query, the error bound is the same, and thus, is as above.\\\\

\subsubsection{Turnstile Case}
The method outlined by Graham Cormode et. al.\footnote{Graham Cormode and S. Muthukrishnan. 2005. What's hot and what's not: tracking most frequent items dynamically. ACM Trans. Database Syst. 30, 1 (March 2005), 249-278. DOI=http://dx.doi.org/10.1145/1061318.106132} is adapted using the count min sketch. In this scheme, a sketch is maintained for each of the $\log n$ dyadic ranges. When an element is inserted or removed, a binary search is preformed while updating the counter at each level of the hierarchy. To find the heavy hitters, a parallel binary search is performed recursively searching each range for which the counter weight exceeds $(\phi + epsilon)||a||_1$. Since at each level there are at most $1/\phi$ items with frequency greater than $\phi$, the probability of failure for each sketch is set at $\delta\phi/(2\log n)$ to limit the number of output items whose true frequency is less than $\phi$. By doing this the total probability that there is any overestimated by more than $\epsilon$ is bounded by $\delta$ by the union bound.

Since the sketch at minimum reports the true count, we are guaranteed that the the true heavy hitters, if they exist, are reported. With probability $1 - \delta$, the algorithm will report a false heavy hitter.

\textbf{Theorem 7:} \\
The algorithm uses space $O((1/\epsilon)\log n \log (2 \log n/(\delta \phi)))$ and time $O(\log n \times \log(2 \log n/(\delta\phi)))$ per update. Every item with frequency at least $(\phi + \epsilon)||a||_1$ is reported, and with probability $1-\delta$ no item whose frequency is less than $\phi ||a||_1$ is output.

% dyadic range - log n versus 2log n
% 1/e < e-d
% getting of the ceil?
% applications?
% background work
% how much detail on proofs we aren't proving?

% Emily - Quantiles 
\end{document}
