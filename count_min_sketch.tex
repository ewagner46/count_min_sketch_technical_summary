\documentclass[11pt]{article}
\usepackage{epsfig}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{theorem}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{bm}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\setlength\parindent{0pt}
\hspace{-2.5pt}}
\newcommand{\wontfix}{\rlap{$\square$}{\large\hspace{1pt}\xmark}}
 \usepackage[parfill]{parskip} 
\usepackage{mathtools}

\newcommand{\sq}{\mathit{Q}_i}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\hypersetup{
    colorlinks   = true,
    linkcolor    = magenta
}

\newcommand{\ra}{\rightarrow}

\title{Technical Summary: ``An Improved Data Stream Summary: The Count-Min Sketch and its Applications'' 
by Cormode and Muthukrishnan}
\author{Ravi Gaddipati, Matthew Ige, and Emily Wagner}
\begin{document}
\maketitle
\section{Problem Statement and Overview}
Consider a vector $\vec{a}(t) = [a_1(t), \dots, a_i(t), \dots a_n(t)]$ which evolves with time.
Initally, $\vec{a}$ is the zero vector $\vec{0}$.  We represent the $t$th update as $(i_t, c_t)$,
which modifies the vector as follows:
\begin{align}
    a_{i_t}(t) = a_{i_t}(t - 1) + c_t, \\
    a_{i'}(t) = a_{i'}(t - 1), \, i' \neq i_t
\end{align}
The update $(i_t, c_t)$ modifies the $i$th element by adding $c_t$ to it.
For all other $a_{i'}$, the vector remains unchanged. 

This vector and its updates represent some stream of data that evolves with
time. There are 2 main models for such a stream:
\begin{enumerate}
    \item \textbf{cash-register case:} $c_t > 0$, so every vector element is monotonically
    increasing.
    \item \textbf{turnstile case:} $c_t$ can also be negative.  There are two subcases:
    \begin{enumerate}
        \item \textbf{non-negative turnstile:} $(i_t, c_t)$ will never cause vector elements
        $a_{i_t}$ to dip below zero. This guarantee can be provided by the application.
        \item \textbf{general turnstile:} vector elements $a_{i_t}$ may become negative.
    \end{enumerate}
\end{enumerate}

The basic problem this work addresses is to summarize or calculate certain characteristics
about the stream. There are 2 main constraints. First, the space used by such algorithms should be small; at most polylogarithmic
in $n$.  This compressed version of the data is called a \textbf{sketch}.  Second, updates to the sketch should be processed quickly.  Since we are sublinear in input space,
we will have to approximate almost any function we want to compute over $\vec{a}$, but
we still want to specify an approximation parameter $\varepsilon$ and bound the probability
of error by $\delta$.  

The \textbf{count-min sketch} addresses these requirements and allows us to calculate several characteristics
of a stream using a single data structure.  It can be used to approximate three types of \textbf{queries}:
\begin{enumerate}
    \item \textbf{point query:} $\sq(i)$ is an approximation of the vector 
    element $a_i(t)$
    \item \textbf{range query:} $\sq(l, r)$ is an approximation of 
    $\sum_{i = l}^{r}a_i$. 
    \item \textbf{inner product query:} $\sq(\vec{a}, \vec{b})$ approximates
    $\vec{a} \odot \vec{b} = \sum_{i = 1}^{n} a_i b_i$
\end{enumerate}

These queries have a variety of applications: point and range queries are used in summarizing
the data distribution and the inner product query can be used to estimate the join size of 
relations. 

In addition, these queries can be used for more complex data stream functions:

\begin{enumerate}
    % not integers
    \item \textbf{$\phi$-quantiles:} The $\phi$-quantiles of the cardinality
    \begin{align}
        ||\vec{a}||_1 = \sum_{i = 1}^{n}|a_i(t)|
    \end{align}
    are the values that split a multiset of integers $I \in [1 \dots n]$ into $\phi$ groups based on
    their rank $R$.  For example, the median of a set of data is the 2-quantile. 
    
    To obtain the $\phi$-quantiles, the elements are sorted and split as
    follows.  The $k$th quantile is the element(s) with rank $R =
    k\phi||\vec{a}||_1$ for $k = 0, \dots, 1/\phi$.  The
    $\varepsilon$-approximation for the $\phi$-quantiles accepts as the $k$-th
    quantile any integer with rank $(k\phi - \varepsilon)||\vec{a}||_1 \leq R
    \leq (k\phi + \varepsilon)||\vec{a}||_1$ for a given $\varepsilon < \phi$.
    \item \textbf{Heavy hitters:}
\end{enumerate}

\section{State of the art}
Sketches have been commonly used to help analyze large streams of data. These sketches allow for the following types of queries: $L_1$ and $L_2$ norms, number of distinct items in a sequence, join size of relations, range sum queries, and more. Although these data structures have proved to be very powerful, there are a few key drawbacks that limit the effectiveness of these sketches. A few of the drawbacks are as follows:
\begin{enumerate}
    \item Common sketches typically have a $\Omega(\frac{1}{\epsilon^2})$ multiplicative factor. For common uses of $\epsilon$, like 0.1 or 0.01, this scaling can quickly become too expensive.
    \item Many sketches take linear time (in the size of the sketch) to do a single update. 
    \item Many sketches require $p$-wise independent hash functions. This is not trivial, and is especially difficult in hardware applications.
    \item Some sketches can only handle one particular query.
    \item Many sketches use analysis that hides large constants.
\end{enumerate}
The count-min sketch reduces these problems in the following ways:
\begin{enumerate}
    \item CMS uses space proportional to $\frac{1}{\epsilon}$
    \item Update time is sublinear with respect to the size
    \item CMS requires only pairwise independent hash functions
    \item The sketch can answer several queries and has numerous applications
    \item All constants are explicit and small.
\end{enumerate}
In particular, notice that the space has been reduce from $\frac{1}{\epsilon^2}$ to $\frac{1}{\epsilon}$ using a CMS, and the time for an update has been reduced from $\frac{1}{\epsilon^2}$ to 1.
\section{How to make a count-min sketch}
Note $e$ is the base of the natural logarithm function, $\ln$. The count-min
sketch is an array with $w$ columns and $d$ rows where:
\begin{align}
    w = \ceil{e/\varepsilon}\\
    d = \ceil{\ln{1/\delta}}
\end{align}
for a given accuracy parameter $\varepsilon$ and probability guarantee $\delta$.
We need $d$ pair-wise independent hash functions
\begin{align}
    h_1 \dots h_d : \{1 \dots n\} \ra \{1 \dots w\} 
\end{align}
The sketch starts with every array entry being 0. The sketch is updated when the $t$th update $\{a_{i_t}, c_t\}$
arrives as follows
\begin{align}
    count[j, h_j(i_t)] \leftarrow count[j, h_j(i_t)] + c_t     
\end{align}
In other words, upon receiving an update $(i_t, c_t)$:
\begin{enumerate}
    \item for each row $1 \leq j \leq d$
    \begin{enumerate}
        \item add $c_t$ to the $h_j(i)$th column.
    \end{enumerate}
\end{enumerate}

\section{Approximate Point Queries using CM sketches}

\subsection{Non-Negative Case}
\subsubsection{Algorithm}
For the non-negative case (either the non-negative turnstile or cash-register case),
the algorithm to estimate $\hat{a}_i(t) = \sq(i)$ is as follows: 
\begin{align}
    \sq(i) = \hat{a}_i = \text{min}_j count[j, h_j(i)]
\end{align}
In other words for a given $i_t = i$, $\hat a_i =$ \texttt{Estimate\_Point\_Query(CM\_Sketch($\vec{a}, t$), i )}:

\begin{lstlisting}[escapeinside={(*}{*)}]
Estimate_Point_Query(CM_Sketch((*$\vec{a}$*), t), i) 
    make empty multiset S;
    for each row j, 1 (*$\leq$*) j (*$\leq$*) d:
        S = S (*$\cup$*) count[j, h_j(i)]
    return min(S);
\end{lstlisting}

\textbf{Theorem 1:} The estimate $\hat a_i$ has the following guarantees:
\begin{enumerate}[label=\textnormal{(\arabic*)}]
    \item $a_i \leq \hat{a}_i$
    \item with probability at least $1 - \delta$, 
    \begin{align}
        \hat{a}_i \leq a_i + \varepsilon ||\vec{a}||_1 
    \end{align}
\end{enumerate}
\subsubsection{Proof of Theorem 1}
\textbf{Define} an indicator variable $I_{i, j, k}$ as follows:
\begin{align}
    I_{i, j, k} = 
    \begin{cases}
        1, & \text{ if } h_j(i) = h_j(k) \text{ and } i \neq k \\
        0, & \text{otherwise}
    \end{cases}
\end{align}
$I_{i, j, k}$ is 1 if $i$ and $k$, $i \neq k$, have a collision under hash function $h_j$.

Since, to begin with, we chose all $h_j$ from a 2-universal hash family
with range $1 \leq h_j \leq d = \ceil{\frac{e}{\varepsilon}}$ 
we know that, for $i \neq k$,
\begin{align}\label{eq:collision-prob}
    Pr(h_j(i) = h_j(k)) \leq \frac{1}{\text{range}(h_j)} = 1/\ceil{\frac{e}{\varepsilon}} = \ceil{\frac{\varepsilon}{e}}
\end{align}
% how do they get rid of ceil??
by the properties of a 2-universal has function and by our choice of hash function
range.

\textbf{Define} $X_{i, j}$ as follows:
\begin{align}
    X_{i, j} = \sum_{k = 1}^{n}I_{i, j, k} a_k
\end{align}
In other words, $X_{i, j}$ is the sum of all corresponding vector values whose indices
collide with the index of $a_i$ under $h_j$. Trivially, we can then state that
\begin{align}\label{eq:count-def}
    count[j, h_j(i)] = a_i + X_{i, j} 
\end{align}
This is clearly true as the column that $i$ hashes to under $h_j$
contains the real value $a_i$ after update $t$, combined with all of the other
$a_k$, $k \neq i$ after update $t$ that hash to that same spot in that row $j$ under
$h_j$.   Now, Theorem 1.a follows because we are only considering the case where vector
elements are non-negative and thus
\begin{align}
    a_i \leq \hat a_i = a_i + X_{i, j}
\end{align}
for any possible $X_{i, j}$

Now, to show 1.b, we evaluate:
\begin{align}
    E(X_{i, j}) = E\left(\sum_{k = 1}^n I_{i, j, k} a_k\right) 
\end{align}
by definition of $X_{i, j}$. The linearity of expectation states that
\begin{align}
    E(\sum_{i = 1}^{n} c_i X_i) = \sum_{i = 1}^{n} c_i E(X_i)
\end{align}
where $X_i$s may be dependent. Applying linearity of expectations here, we have:
\begin{align}
    E(\sum_{k = 1}^{n} a_k I_{i, j, k}) = \sum_{k = 1}^{n} a_k E(I_{i, j, k}) 
\end{align}
% why less than equals, why not just equals?
We calculated $E(I_{i, j, k}) \leq \ceil{\frac{\varepsilon}{e}}$ above, so 
we have
\begin{align}
    \sum_{k = 1}^{n} a_k E(I_{i, j, k})  \leq\sum_{k = 1}^{n} a_k \ceil{\frac{\varepsilon}{e}}
\end{align}
and finally, since $||\vec{a}||_1 = \sum_{k = 1}^{n} |a_k| = \sum_{k = 1}^{n} a_k$
(all $a_k$ are assumed to be non-negative)
\begin{align}
    \sum_{k = 1}^{n} a_k \ceil{\frac{\varepsilon}{e}} = \ceil{\frac{\varepsilon}{e}} ||\vec{a}|||_1 \\
    \implies E(X_{i, j}) \leq \ceil{\frac{\varepsilon}{e}} ||\vec{a}||_1
\end{align}
% why less than equals, why not just equals?
Now that we know that this is true in expectation, we use Markov to bound the tail bounds:
\begin{align}
    Pr[X \geq a] = \frac{1}{a}E(X) 
\end{align}
Let $X = \hat{a}_i$, and choose $a = a_i + \varepsilon||a||_1$.  Then,
\begin{align}
    Pr[\hat{a}_i > a_i + \varepsilon ||a||_1 = Pr[\forall_{j \cdot} count[j, h_j(i)] > a_i + \varepsilon ||\vec{a}||_1] 
\end{align}
since the probability that all values are greater than $a_i + \varepsilon||\vec a||_1$ is equivalent 
to the probability that the minimum of all values is greater than $a_i + \varepsilon||\vec a||_1$,
which is the definition of $\hat a_i$. Then, as stated in equation \ref{eq:count-def}, we have
\begin{align}
    Pr[\forall_{j \cdot} count[j, h_j(i)] > a_i + \varepsilon ||\vec{a}||_1] = Pr[\forall_{j \cdot} a_i + X_{i, j} > a_i + \varepsilon ||\vec{a}||_1]
\end{align}
Then, subtracting $a_i$ from both sides, and using equation \ref{eq:collision-prob} on the $\varepsilon ||\vec{a}||_1$ term, we have:
\begin{align}
    Pr[\forall_{j \cdot} a_i + X_{i, j} > a_i + \varepsilon ||\vec{a}||_1]  = Pr[\forall_{j \cdot} X_{i, j} > e E(X_{i, j})]
\end{align}
Finally, applying the bounds given by Markov's inequality, we have:
\begin{align}
      Pr[\forall_{j \cdot} X_{i, j} > e E(X_{i, j})] \leq \frac{E(X_{i, j})}{a_i + \varepsilon||a||_1} < e^{-d} < \delta
\end{align}
This estimate is calculated in $O(\ln \frac{1}{\delta})$ as the minimum of a multiset can be taken in linear time.
The space complexity is $(2 + \frac{e}{\varepsilon})\ln \frac{1}{\delta}$ words.

All previous analyses of sketch algorithm use Chebyshev in their estimation analysis, yielding a dependency on
$\frac{1}{\epsilon^2}$ for the space complexity.  Using Markov in this analysis yields a tighter bound,
with a dependency $\frac{1}{\epsilon}$.

Considering only the non-negative case here confers 2 advantages as we can take the minimum of the estimate,
rather than the median.  This allows us only one-sided error, and allows us to calculate the failure
probability, instead of using Chernoff bounds, yielding smaller constants than prior work.

\subsection{General Case}
\subsubsection{Algorithm}
The algorithm is identical to the non-negative case, except that we take the median of the multiset:
\begin{align}
    \sq(i) = \hat{a}_i = \text{median}_j count[j, h_j(i)]
\end{align}

\textbf{Theorem 2:} With probability $1 - \delta^{1/4}$,
\begin{align}
    a_i - 3\varepsilon||\vec{a}||_1 \leq \hat{a}_i \leq a_i + 3\varepsilon||\vec{a}||_1. 
\end{align}
The time and space complexity are the same as the non-negative case: $O(\ln
\frac{1}{\delta})$ time and $(2 + \frac{e}{\varepsilon})\ln \frac{1}{\delta}$
words, respectively.

\section{Range query:}
A range query, denoted as: $\mathcal{Q}(l,r)$, requests the value:
\begin{align*}
    a[l,4] = \Sigma_{i=l}^r a_i
    \end{align*}
    And we denote our estimator based on the CM sketch as: $\hat a[l,r]$.\\\\
    \textbf{Procedure for estimation:}\\
    Keep $log_2n$ CM sketches. A single range query can be converted into $2log_2n$ dyadic range queries, and each dyadic range query can be converted into a single point query. A CM sketch is kept for every dyadic range, and we do a single point query for every dyadic range. The sum of all of the point queries is the result, $\hat a[l,r]$.\\\\
    TODO better explain dyadic range stuff? ASK LIN \\\\
    \textbf{Theorem 4: } $a[l,r] \leq \hat a[l,r]$. With probability at least $(1-\delta)$, $\hat a[l,r] \leq a[l,r] + 2\epsilon logn ||\bm{a}||_1$.\\\\
    \textbf{Proof:}\\
    TODO explain why we have 2log2n dyadic ranges, which are equivalently a point query for each. Then:\\
    This proof is a simple extension of the point query proof. Because a single range query can be transformed to $2\log_2n$ point queries, we can extend the error bounds of many point queries. Recall that a single point query errors by at most $\epsilon ||\bm{a}||_1$, with probability $\delta$. Because each of these point queries that we used are taken from a separate CM sketch, they are all independent. Thus, we can simply add the errors together, and therefore, we error by at most $2\log_2n(\epsilon||\bm{a}||_1)$, and with the same probability, $\delta$.

\section{Applications}

\subsection{Quantiles in the turnstile model}
The count-min sketch can be used to calculate the $\phi$-quantiles in the turnstile model.
Prior work (21) shows that $\phi$-quantiles can be approximated using range sums. This is done
as follows:
\begin{enumerate}
    \item For each $k \in {1, 2, \dots, 1/\phi}$
    \begin{enumerate}
        \item Find a range sum such that $a[1, r] = k\phi||\vec{a}||_1$. Find $r$ using a binary
        search on the possible range sums $a[1, \hat{r}]$, where $\hat{r} \in \{1, 2, \dots, n\}$.  $r$ is the
        $\varepsilon$-approximation for the $k$th $\phi$-quantile.
    \end{enumerate}
\end{enumerate}
The method in [21] uses Random Subset Sums to approximate range sums.  If instead a count-min
sketch is used to approximate the range sums, better results follow.  

To use a count-min sketch to approximate quantiles in the turnstile model, $\log n$ sketches are kept,
one for each dyadic range. Each sketch gets accuracy parameter $\varepsilon/\log n$ for an overall accuracy
bounded by $\varepsilon$. Each sketch gets probability guarantee $\delta\phi/\log n$, for
an overall probability guarantee of $\delta$ for all $1/\phi$ quantiles. Then, we have the following:

\textbf{Theorem 5:} $\varepsilon$-approximate $\phi$-quantiles can be found with probability at least
$1 - \delta$ by keeping a data structure with space $O\left(\frac{1}{\varepsilon}
\log^2(n) \log \left(\frac{\log n}{\phi \delta}\right)\right)$ The time for each insert or delete operation is
$O\left(\log(n) \log \left(\frac{\log n}{\phi \delta}\right)\right)$, and the time to find each quantile on demand 
is $O\left(\log(n)\log\left(\frac{\log n}{\phi \delta}\right)\right)$

This improves the existing query time and update by a factor of more than $\frac{34}{\varepsilon^2} \log n$
The space requirements are improved by a factor of at least $\frac{34}{\varepsilon}$. 

\subsection{Heavy hitters for the cash register case}
Recall that the cash register case is for positive updates only. We can easily obtain $||\bm{a}||$ at any given point in time because it is simply: $\Sigma_{i=1}^t c_i$.We define a $\phi$ heavy hitter if the estimation of the point query, $Q(i_t) =  \hat a_{it} \geq \phi ||\bm{a}(t))||$. We can do this by maintaining a heap to store the items above the $phi||\bm{a}(t)||$ threshold. On any update, we check the lowest value in the heap, and if the update would be greater than the lowest item, we replace it in the heap. When we are finished with the stream, we do a final scan of all items in the heap, and return the ones which have a value over $\phi||\bm{a}||_1$.\\\\
\textbf{Theorem 6: } We can identify the heavy hitters of a sequence of length $||\bm{a_i}||_1$ using space $O(\frac{1}{\epsilon}log(\frac{||\bm{a}||}{\delta}))$ and time $O(log(\frac{||\bm{a}||}{\delta}))$. Every item which occurs with count $\phi ||\bm{a}||_1$ is output, and with probability $(1-\delta)$, no items whose count is less than $(\phi - \epsilon)||\bm{a}||_1$ is output.\\\\
\textbf{Proof idea:}\\
Because we have only positive updates, and a CM sketch will only over estimate, it is not possible to miss any heavy hitter. Thus, we will never omit any heavy hitters. The bound on the error of outputting a non heavy hitter comes from the fact that a single point query outputs the estimate count of an item within $\epsilon$ of the actual value, with probabiltiy $(1-\delta)$. Because the heavy hitter relies purely on the point query, the error bound is the same, and thus, is as above.\\\\

% dyadic range - log n versus 2log n
% 1/e < e-d
% getting of the ceil?
% applications?
% background work
% how much detail on proofs we aren't proving?

% Emily - Quantiles 
\end{document}
