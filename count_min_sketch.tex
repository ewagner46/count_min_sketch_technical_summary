\documentclass[11pt]{article}
\usepackage{epsfig}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{theorem}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{titlesec}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\setlength\parindent{0pt}
\hspace{-2.5pt}}
\newcommand{\wontfix}{\rlap{$\square$}{\large\hspace{1pt}\xmark}}
 \usepackage[parfill]{parskip} 
\usepackage{mathtools}

\newcommand{\sq}{\mathit{Q}_i}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\hypersetup{
    colorlinks   = true,
    linkcolor    = magenta
}

\newcommand{\ra}{\rightarrow}

\title{Technical Summary: ``An Improved Data Stream Summary: The Count-Min Sketch and its Applications'' 
by Cormode and Muthukrishnan}
\author{Ravi Gaddipati, Matthew Ige, and Emily Wagner}
\begin{document}
\maketitle
\section{Problem Statement and Overview}
Consider a vector $\vec{a}(t) = [a_1(t), \dots, a_i(t), \dots a_n(t)]$ which evolves with time.
Initally, $\vec{a}$ is the zero vector $\vec{0}$.  We represent the $t$th update as $(i_t, c_t)$,
which modifies the vector as follows:
\begin{align}
    a_{i_t}(t) = a_{i_t}(t - 1) + c_t, \\
    a_{i'}(t) = a_{i'}(t - 1), \, i' \neq i_t
\end{align}
The update $(i_t, c_t)$ modifies the $i$th element by adding $c_t$ to it.
For all other $a_{i'}$, the vector remains unchanged. 

This vector and its updates represent some stream of data that evolves with
time. There are 2 main models for such a stream:
\begin{enumerate}
    \item \textbf{cash-register case:} $c_t > 0$, so every vector element is monotonically
    increasing.
    \item \textbf{turnstile case:} $c_t$ can also be negative.  There are two subcases:
    \begin{enumerate}
        \item \textbf{non-negative turnstile:} $(i_t, c_t)$ will never cause vector elements
        $a_{i_t}$ to dip below zero. This guarantee can be provided by the application.
        \item \textbf{general turnstile:} vector elements $a_{i_t}$ may become negative.
    \end{enumerate}
\end{enumerate}

The basic problem this work addresses is to summarize or calculate certain characteristics
about the stream. There are 2 main constraints. First, the space used by such algorithms should be small; at most polylogarithmic
in $n$.  This compressed version of the data is called a \textbf{sketch}.  Second, updates to the sketch should be processed quickly.  Since we are sublinear in input space,
we will have to approximate almost any function we want to compute over $\vec{a}$, but
we still want to specify an approximation parameter $\varepsilon$ and bound the probability
of error by $\delta$.  

The \textbf{count-min sketch} addresses these requirements and allows us to calculate several characteristics
of a stream using a single data structure.  It can be used to approximate three types of \textbf{queries}:
\begin{enumerate}
    \item \textbf{point query:} $\sq(i)$ is an approximation of the vector 
    element $a_i(t)$
    \item \textbf{range query:} $\sq(l, r)$ is an approximation of 
    $\sum_{i = l}^{r}a_i$. 
    \item \textbf{inner product query:} $\sq(\vec{a}, \vec{b})$ approximates
    $\vec{a} \odot \vec{b} = \sum_{i = 1}^{n} a_i b_i$
\end{enumerate}

These queries have a variety of applications: point and range queries are used in summarizing
the data distribution and the inner product query can be used to estimate the join size of 
relations. 

In addition, these queries can be used for more complex data stream functions:

\begin{enumerate}
    % not integers
    \item \textbf{$\phi$-quantiles:} The $\phi$-quantiles of the cardinality
    \begin{align}
        ||\vec{a}||_1 = \sum_{i = 1}^{n}|a_i(t)|
    \end{align}
    are the values that split a multiset of integers $I \in [1 \dots n]$ into $\phi$ groups based on
    their rank $R$.  For example, the median of a set of data is the 2-quantile. 
    
    To obtain the $\phi$-quantiles, the elements are sorted and split as
    follows.  The $k$th quantile is the element(s) with rank $R =
    k\phi||\vec{a}||_1$ for $k = 0, \dots, 1/\phi$.  The
    $\varepsilon$-approximation for the $\phi$-quantiles accepts as the $k$-th
    quantile any integer with rank $(k\phi - \varepsilon)||\vec{a}||_1 \leq R
    \leq (k\phi + \varepsilon)||\vec{a}||_1$ for a given $\varepsilon < \phi$.
    \item \textbf{Heavy hitters:}
\end{enumerate}

\section{How to make a count-min sketch}
Note $e$ is the base of the natural logarithm function, $\ln$. The count-min
sketch is an array with $w$ columns and $d$ rows where:
\begin{align}
    w = \ceil{e/\varepsilon}\\
    d = \ceil{\ln{1/\delta}}
\end{align}
for a given accuracy parameter $\varepsilon$ and probability guarantee $\delta$.
We need $d$ pair-wise independent hash functions
\begin{align}
    h_1 \dots h_d : \{1 \dots n\} \ra \{1 \dots w\} 
\end{align}
The sketch starts with every array entry being 0. The sketch is updated when the $t$th update $\{a_{i_t}, c_t\}$
arrives as follows
\begin{align}
    count[j, h_j(i_t)] \leftarrow count[j, h_j(i_t)] + c_t     
\end{align}
In other words, upon receiving an update $(i_t, c_t)$:
\begin{enumerate}
    \item for each row $1 \leq j \leq d$
    \begin{enumerate}
        \item add $c_t$ to the $h_j(i)$th column.
    \end{enumerate}
\end{enumerate}

\section{Approximate Point Queries using CM sketches}

\subsection{Non-Negative Case}
\subsubsection{Algorithm}
For the non-negative case (either the non-negative turnstile or cash-register case),
the algorithm to estimate $\hat{a}_i(t) = \sq(i)$ is as follows: 
\begin{align}
    \sq(i) = \hat{a}_i = \text{min}_j count[j, h_j(i)]
\end{align}
In other words for a given $i_t = i$, $\hat a_i =$ \texttt{Estimate\_Point\_Query(CM\_Sketch($\vec{a}, t$), i )}:

\begin{lstlisting}[escapeinside={(*}{*)}]
Estimate_Point_Query(CM_Sketch((*$\vec{a}$*), t), i) 
    make empty multiset S;
    for each row j, 1 (*$\leq$*) j (*$\leq$*) d:
        S = S (*$\cup$*) count[j, h_j(i)]
    return min(S);
\end{lstlisting}

\textbf{Theorem 1:} The estimate $\hat a_i$ has the following guarantees:
\begin{enumerate}[label=\textnormal{(\arabic*)}]
    \item $a_i \leq \hat{a}_i$
    \item with probability at least $1 - \delta$, 
    \begin{align}
        \hat{a}_i \leq a_i + \varepsilon ||\vec{a}||_1 
    \end{align}
\end{enumerate}
\subsubsection{Proof of Theorem 1}
\textbf{Define} an indicator variable $I_{i, j, k}$ as follows:
\begin{align}
    I_{i, j, k} = 
    \begin{cases}
        1, & \text{ if } h_j(i) = h_j(k) \text{ and } i \neq k \\
        0, & \text{otherwise}
    \end{cases}
\end{align}
$I_{i, j, k}$ is 1 if $i$ and $k$, $i \neq k$, have a collision under hash function $h_j$.

Since, to begin with, we chose all $h_j$ from a 2-universal hash family
with range $1 \leq h_j \leq d = \ceil{\frac{e}{\varepsilon}}$ 
we know that, for $i \neq k$,
\begin{align}\label{eq:collision-prob}
    Pr(h_j(i) = h_j(k)) \leq \frac{1}{\text{range}(h_j)} = 1/\ceil{\frac{e}{\varepsilon}} = \ceil{\frac{\varepsilon}{e}}
\end{align}
% how do they get rid of ceil??
by the properties of a 2-universal has function and by our choice of hash function
range.

\textbf{Define} $X_{i, j}$ as follows:
\begin{align}
    X_{i, j} = \sum_{k = 1}^{n}I_{i, j, k} a_k
\end{align}
In other words, $X_{i, j}$ is the sum of all corresponding vector values whose indices
collide with the index of $a_i$ under $h_j$. Trivially, we can then state that
\begin{align}\label{eq:count-def}
    count[j, h_j(i)] = a_i + X_{i, j} 
\end{align}
This is clearly true as the column that $i$ hashes to under $h_j$
contains the real value $a_i$ after update $t$, combined with all of the other
$a_k$, $k \neq i$ after update $t$ that hash to that same spot in that row $j$ under
$h_j$.   Now, Theorem 1.a follows because we are only considering the case where vector
elements are non-negative and thus
\begin{align}
    a_i \leq \hat a_i = a_i + X_{i, j}
\end{align}
for any possible $X_{i, j}$

Now, to show 1.b, we evaluate:
\begin{align}
    E(X_{i, j}) = E\left(\sum_{k = 1}^n I_{i, j, k} a_k\right) 
\end{align}
by definition of $X_{i, j}$. The linearity of expectation states that
\begin{align}
    E(\sum_{i = 1}^{n} c_i X_i) = \sum_{i = 1}^{n} c_i E(X_i)
\end{align}
where $X_i$s may be dependent. Applying linearity of expectations here, we have:
\begin{align}
    E(\sum_{k = 1}^{n} a_k I_{i, j, k}) = \sum_{k = 1}^{n} a_k E(I_{i, j, k}) 
\end{align}
% why less than equals, why not just equals?
We calculated $E(I_{i, j, k}) \leq \ceil{\frac{\varepsilon}{e}}$ above, so 
we have
\begin{align}
    \sum_{k = 1}^{n} a_k E(I_{i, j, k})  \leq\sum_{k = 1}^{n} a_k \ceil{\frac{\varepsilon}{e}}
\end{align}
and finally, since $||\vec{a}||_1 = \sum_{k = 1}^{n} |a_k| = \sum_{k = 1}^{n} a_k$
(all $a_k$ are assumed to be non-negative)
\begin{align}
    \sum_{k = 1}^{n} a_k \ceil{\frac{\varepsilon}{e}} = \ceil{\frac{\varepsilon}{e}} ||\vec{a}|||_1 \\
    \implies E(X_{i, j}) \leq \ceil{\frac{\varepsilon}{e}} ||\vec{a}||_1
\end{align}
% why less than equals, why not just equals?
Now that we know that this is true in expectation, we use Markov to bound the tail bounds:
\begin{align}
    Pr[X \geq a] = \frac{1}{a}E(X) 
\end{align}
Let $X = \hat{a}_i$, and choose $a = a_i + \varepsilon||a||_1$.  Then,
\begin{align}
    Pr[\hat{a}_i > a_i + \varepsilon ||a||_1 = Pr[\forall_{j \cdot} count[j, h_j(i)] > a_i + \varepsilon ||\vec{a}||_1] 
\end{align}
since the probability that all values are greater than $a_i + \varepsilon||\vec a||_1$ is equivalent 
to the probability that the minimum of all values is greater than $a_i + \varepsilon||\vec a||_1$,
which is the definition of $\hat a_i$. Then, as stated in equation \ref{eq:count-def}, we have
\begin{align}
    Pr[\forall_{j \cdot} count[j, h_j(i)] > a_i + \varepsilon ||\vec{a}||_1] = Pr[\forall_{j \cdot} a_i + X_{i, j} > a_i + \varepsilon ||\vec{a}||_1]
\end{align}
Then, subtracting $a_i$ from both sides, and using equation \ref{eq:collision-prob} on the $\varepsilon ||\vec{a}||_1$ term, we have:
\begin{align}
    Pr[\forall_{j \cdot} a_i + X_{i, j} > a_i + \varepsilon ||\vec{a}||_1]  = Pr[\forall_{j \cdot} X_{i, j} > e E(X_{i, j})]
\end{align}
Finally, applying the bounds given by Markov's inequality, we have:
\begin{align}
      Pr[\forall_{j \cdot} X_{i, j} > e E(X_{i, j})] \leq \frac{E(X_{i, j})}{a_i + \varepsilon||a||_1} < e^{-d} < \delta
\end{align}
This estimate is calculated in $O(\ln \frac{1}{\delta})$ as the minimum of a multiset can be taken in linear time.
The space complexity is $(2 + \frac{e}{\varepsilon})\ln \frac{1}{\delta}$ words.

All previous analyses of sketch algorithm use Chebyshev in their estimation analysis, yielding a dependency on
$\frac{1}{\epsilon^2}$ for the space complexity.  Using Markov in this analysis yields a tighter bound,
with a dependency $\frac{1}{\epsilon}$.

Considering only the non-negative case here confers 2 advantages as we can take the minimum of the estimate,
rather than the median.  This allows us only one-sided error, and allows us to calculate the failure
probability, instead of using Chernoff bounds, yielding smaller constants than prior work.

\subsection{General Case}
\subsubsection{Algorithm}
The algorithm is identical to the non-negative case, except that we take the median of the multiset:
\begin{align}
    \sq(i) = \hat{a}_i = \text{median}_j count[j, h_j(i)]
\end{align}

\textbf{Theorem 2:} With probability $1 - \delta^{1/4}$,
\begin{align}
    a_i - 3\varepsilon||\vec{a}||_1 \leq \hat{a}_i \leq a_i + 3\varepsilon||\vec{a}||_1. 
\end{align}
The time and space complexity are the same as the non-negative case: $O(\ln
\frac{1}{\delta})$ time and $(2 + \frac{e}{\varepsilon})\ln \frac{1}{\delta}$
words, respectively.

\section{Applications}

\subsection{Quantiles in the turnstile model}
The count-min sketch can be used to calculate the $\phi$-quantiles in the turnstile model.
Prior work (21) shows that $\phi$-quantiles can be approximated using range sums. This is done
as follows:
\begin{enumerate}
    \item For each $k \in {1, 2, \dots, 1/\phi}$
    \begin{enumerate}
        \item Find a range sum such that $a[1, r] = k\phi||\vec{a}||_1$. Find $r$ using a binary
        search on the possible range sums $a[1, \hat{r}]$, where $\hat{r} \in \{1, 2, \dots, n\}$.  $r$ is the
        $\varepsilon$-approximation for the $k$th $\phi$-quantile.
    \end{enumerate}
\end{enumerate}
The method in [21] uses Random Subset Sums to approximate range sums.  If instead a count-min
sketch is used to approximate the range sums, better results follow.  

To use a count-min sketch to approximate quantiles in the turnstile model, $\log n$ sketches are kept,
one for each dyadic range. Each sketch gets accuracy parameter $\varepsilon/\log n$ for an overall accuracy
bounded by $\varepsilon$. Each sketch gets probability guarantee $\delta\phi/\log n$, for
an overall probability guarantee of $\delta$ for all $1/\phi$ quantiles. Then, we have the following:

\textbf{Theorem 5:} $\varepsilon$-approximate $\phi$-quantiles can be found with probability at least
$1 - \delta$ by keeping a data structure with space $O\left(\frac{1}{\varepsilon}
\log^2(n) \log \left(\frac{\log n}{\phi \delta}\right)\right)$ The time for each insert or delete operation is
$O\left(\log(n) \log \left(\frac{\log n}{\phi \delta}\right)\right)$, and the time to find each quantile on demand 
is $O\left(\log(n)\log\left(\frac{\log n}{\phi \delta}\right)\right)$

This improves the existing query time and update by a factor of more than $\frac{34}{\varepsilon^2} \log n$
The space requirements are improved by a factor of at least $\frac{34}{\varepsilon}$. 
% dyadic range - log n versus 2log n
% 1/e < e-d
% getting of the ceil?
% applications?
% background work
% how much detail on proofs we aren't proving?

% Emily - Quantiles 
\end{document}
